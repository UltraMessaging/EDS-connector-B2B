/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package com.infa.presales.spark;

import com.google.common.io.Closeables;
//import com.latencybusters.lbm.*;

import org.apache.log4j.Logger;
import org.apache.log4j.Level;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function2;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.storage.StorageLevel;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.receiver.Receiver;
import scala.Tuple2;

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.ConnectException;
import java.net.Socket;
import java.nio.charset.StandardCharsets;
import java.util.Arrays;
import java.util.Iterator;
//import java.util.Iterator;
import java.util.regex.Pattern;

/**
 * Custom Receiver that receives data over a socket. Received bytes is
 * interpreted as text and \n delimited lines are considered as records. They
 * are then counted and printed.
 *
 * Usage: JavaCustomReceiver <master> <hostname> <port> <master> is the Spark
 * master URL. In local mode, <master> should be 'local[n]' with n > 1.
 * <hostname> and <port> of the TCP server that Spark Streaming would connect to
 * receive data.
 *
 * To run this on your local machine, you need to first run a Netcat server `$
 * nc -lk 9999` and then run the example `$ bin/run-example
 * org.apache.spark.examples.streaming.JavaCustomReceiver localhost 9999`
 */

public class TestSparkReceiver extends Receiver<String> {
	/**
	 * 
	 */
	private static final long serialVersionUID = 1L;
	private static final Pattern SPACE = Pattern.compile(" ");
	static String m_appName = "";

	static Iterable<String> iterable(final Iterator<String> iterator) {
		if (iterator == null) {
			throw new NullPointerException();
		}
		return new Iterable<String>() {
			public Iterator<String> iterator() {
				return iterator;
			}
		};
	}
	static PairFunction<String, String, Integer> pairFunc = new PairFunction<String, String, Integer>() {
		/**
		 * 
		 */
		private static final long serialVersionUID = 1L;

		@Override
		public Tuple2<String, Integer> call(String s) {
			return new Tuple2<>(s, 1);
		}
	};
	
	static FlatMapFunction<String, String> flatMapFunc = new FlatMapFunction<String, String>() {
		/**
		 * 
		 */
		private static final long serialVersionUID = 1L;

		// Changed return type from Iterator<String>
		@Override
		public Iterable<String> call(String x) {
			Iterable<String> iter = iterable( Arrays.asList(SPACE.split(x)).iterator());;
			return  iter;
		}
	};
	
	static Function2<Integer, Integer, Integer> func2 = new Function2<Integer, Integer, Integer>() {
		/**
		 * 
		 */
		private static final long serialVersionUID = 1L;

		@Override
		public Integer call(Integer i1, Integer i2) {
			return i1 + i2;
		}
	};
	private static Logger _logger;

	public static void main(String[] args) throws Exception {
		m_appName = Thread.currentThread().getStackTrace()[1].getClassName();
		if (args.length < 2) {
			System.err.println("Usage: JavaCustomReceiver <hostname> <port>");
			System.exit(1);
		}

		setStreamingLogLevels();

		// Create the context with a 1 second batch size
		
		SparkConf sparkConf = new SparkConf().setAppName(m_appName);
		JavaStreamingContext ssc = new JavaStreamingContext(sparkConf, new Duration(1000));

		// Create an input stream with the custom receiver on target ip:port and
		// count the
		// words in input stream of \n delimited text (eg. generated by 'nc')
		TestSparkReceiver umSparkRcv = new TestSparkReceiver(args);
		JavaReceiverInputDStream<String> lines = ssc.receiverStream(umSparkRcv);
		JavaDStream<String> words = lines.flatMap(flatMapFunc);
		JavaPairDStream<String, Integer> wordCounts = words.mapToPair(pairFunc).reduceByKey(func2);

		wordCounts.print();
		ssc.start();
		ssc.awaitTermination();
		ssc.close();
	}

	// ============= Receiver code that receives data over a socket
	// ==============

	String host = null;
	int port = -1;

	public TestSparkReceiver(String[] args) {
		super(StorageLevel.MEMORY_AND_DISK_2());
		host = args[0];
		port = Integer.parseInt(args[1]);
	}

	public void onStart() {
		// Start the thread that receives data over a connection
		new Thread() {
			@Override
			public void run() {
				try {
				receive();
				} catch (Exception e) {
					_logger.warn("Run Exception", e);
				}
			}
		}.start();
	}

	public void onStop() {
		// There is nothing much to do as the thread calling receive()
		// is designed to stop by itself isStopped() returns false
	}

	/** Create a socket connection and receive data until receiver is stopped */
	private void receive() throws Exception {
		try {
			Socket socket = null;
			BufferedReader reader = null;
			String userInput = null;
			try {
				// connect to the server
				socket = new Socket(host, port);
				reader = new BufferedReader(new InputStreamReader(socket.getInputStream(), StandardCharsets.UTF_8));
				// Until stopped or connection broken continue reading
				while (!isStopped() && (userInput = reader.readLine()) != null) {
					System.out.println("Received data '" + userInput + "'");
					store(userInput);
				}
			} finally {
				Closeables.close(reader, /* swallowIOException = */ true);
				Closeables.close(socket, /* swallowIOException = */ true);
			}
			// Restart in an attempt to connect again when server is active
			// again
			restart("Trying to connect again");
		} catch (ConnectException ce) {
			// restart if could not connect to server
			restart("Could not connect", ce);
		} catch (Throwable t) {
			restart("Error receiving data", t);
		}
	}

	static void setStreamingLogLevels() {
		_logger = Logger.getRootLogger();
		boolean log4jInitialized = _logger.getAllAppenders().hasMoreElements();
		if (!log4jInitialized) {
			// We first log something to initialize Spark's default logging,
			// then we override the
			// logging level.
			_logger.warn("Setting log level to [WARN] for streaming example."
					+ " To override add a custom log4j.properties to the classpath.");
			_logger.setLevel(Level.WARN);
		}
	}
	
}
